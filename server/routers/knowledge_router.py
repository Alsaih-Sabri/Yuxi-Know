import asyncio
import os
import textwrap
import traceback
from urllib.parse import quote, unquote

import aiofiles
from fastapi import APIRouter, Body, Depends, File, HTTPException, Query, Request, UploadFile
from fastapi.responses import FileResponse
from starlette.responses import StreamingResponse

from server.services.tasker import TaskContext, tasker
from server.utils.auth_middleware import get_admin_user
from src import config, knowledge_base
from src.knowledge.indexing import SUPPORTED_FILE_EXTENSIONS, is_supported_file_extension, process_file_to_markdown
from src.knowledge.utils import calculate_content_hash
from src.models.embed import test_all_embedding_models_status, test_embedding_model_status
from src.models import select_model
from src.storage.db.models import User
from src.storage.minio.client import StorageError, aupload_file_to_minio, get_minio_client
from src.utils import logger

knowledge = APIRouter(prefix="/knowledge", tags=["knowledge"])


# =============================================================================
# === Helper Functions ===
# =============================================================================

async def _auto_generate_mindmap(db_id: str):
    """
    Auto-generate mindmap for a knowledge base in the background
    
    Args:
        db_id: Database ID
    """
    try:
        import json
        
        # Get database info
        db_info = knowledge_base.get_database_info(db_id)
        if not db_info:
            logger.warning(f"Database {db_id} not found for mindmap generation")
            return
        
        db_name = db_info.get("name", "Knowledge Base")
        all_files = db_info.get("files", {})
        
        # Get file list (limit to 20 files)
        file_ids = list(all_files.keys())[:20]
        if not file_ids:
            logger.info(f"No files to generate mindmap for database {db_id}")
            return
        
        # Collect file info
        files_info = []
        for file_id in file_ids:
            if file_id in all_files:
                file_info = all_files[file_id]
                files_info.append({
                    "filename": file_info.get("filename", ""),
                    "type": file_info.get("type", ""),
                })
        
        if not files_info:
            return
        
        # Build AI prompt
        system_prompt = """You are a professional knowledge organization assistant.

Your task is to analyze the provided file list and generate a clear hierarchical mindmap structure.

**IMPORTANT: Generate all category names in ENGLISH. Only keep original filenames as they are.**

**Core rule: Each filename can only appear once! No duplicates allowed!**

Requirements:
1. Clear hierarchical structure (2-4 levels)
2. Root node is the knowledge base name (translate to English if needed)
3. First level is main categories in ENGLISH (e.g., Technical Docs, Regulations, Data Resources)
4. Second level is subcategories in ENGLISH
5. **Leaf nodes must be specific filenames (keep original names)**
6. **Each filename can only appear once in the entire mindmap!**
7. If a file could belong to multiple categories, choose only the most appropriate one
8. Use appropriate emoji icons for better readability
9. Return JSON format following this structure:

```json
{
  "content": "Knowledge Base Name",
  "children": [
    {
      "content": "ğŸ¯ Main Category 1",
      "children": [
        {
          "content": "Subcategory 1.1",
          "children": [
            {"content": "file1.txt", "children": []},
            {"content": "file2.pdf", "children": []}
          ]
        }
      ]
    }
  ]
}
```

**Important constraints:**
- Each filename can only appear once in the entire JSON
- Don't classify by multiple dimensions causing file duplication
- Choose the most appropriate classification dimension
- Each leaf node's children must be an empty array []
- Category names should be concise and clear
- Use emojis to enhance visual effect"""
        
        files_text = "\n".join([f"- {f['filename']} ({f['type']})" for f in files_info])
        user_message = f"""Please generate a mindmap structure for knowledge base "{db_name}".

File list ({len(files_info)} files):
{files_text}

**Important reminder:**
1. This knowledge base has {len(files_info)} files
2. Each filename can only appear once in the mindmap
3. Don't let the same file appear in multiple categories
4. Choose the most appropriate unique category for each file
5. **Generate ALL category names in ENGLISH** (only keep original filenames unchanged)

Please generate a reasonable mindmap structure with English category names."""
        
        # Call AI to generate
        logger.info(f"Generating mindmap for {db_name} with {len(files_info)} files")
        
        model = select_model()
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        response = await asyncio.to_thread(model.call, messages, stream=False)
        
        # Parse AI response
        content = response.content if hasattr(response, "content") else str(response)
        
        # Extract JSON from markdown code block
        if "```json" in content:
            json_start = content.find("```json") + 7
            json_end = content.find("```", json_start)
            content = content[json_start:json_end].strip()
        elif "```" in content:
            json_start = content.find("```") + 3
            json_end = content.find("```", json_start)
            content = content[json_start:json_end].strip()
        
        mindmap_data = json.loads(content)
        
        # Validate structure
        if not isinstance(mindmap_data, dict) or "content" not in mindmap_data:
            logger.error("Invalid mindmap structure generated")
            return
        
        # Save mindmap to knowledge base metadata
        async with knowledge_base._metadata_lock:
            if db_id in knowledge_base.global_databases_meta:
                knowledge_base.global_databases_meta[db_id]["mindmap"] = mindmap_data
                knowledge_base._save_global_metadata()
                logger.info(f"Auto-generated mindmap saved for database {db_id}")
        
    except Exception as e:
        logger.error(f"Failed to auto-generate mindmap for {db_id}: {e}")


# =============================================================================
# === API Endpoints ===
# =============================================================================

media_types = {
    ".pdf": "application/pdf",
    ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    ".txt": "text/plain",
    ".md": "text/markdown",
    ".json": "application/json",
    ".csv": "text/csv",
    ".xlsx": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    ".xls": "application/vnd.ms-excel",
    ".pptx": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
    ".ppt": "application/vnd.ms-powerpoint",
    ".jpg": "image/jpeg",
    ".jpeg": "image/jpeg",
    ".png": "image/png",
    ".gif": "image/gif",
    ".bmp": "image/bmp",
    ".svg": "image/svg+xml",
    ".zip": "application/zip",
    ".rar": "application/x-rar-compressed",
    ".7z": "application/x-7z-compressed",
    ".tar": "application/x-tar",
    ".gz": "application/gzip",
    ".html": "text/html",
    ".htm": "text/html",
    ".xml": "text/xml",
    ".css": "text/css",
    ".js": "application/javascript",
    ".py": "text/x-python",
    ".java": "text/x-java-source",
    ".cpp": "text/x-c++src",
    ".c": "text/x-csrc",
    ".h": "text/x-chdr",
    ".hpp": "text/x-c++hdr",
}

# =============================================================================
# === çŸ¥è¯†åº“ç®¡ç†åˆ†ç»„ ===
# =============================================================================


@knowledge.get("/databases")
async def get_databases(current_user: User = Depends(get_admin_user)):
    """è·å–æ‰€æœ‰çŸ¥è¯†åº“"""
    try:
        database = knowledge_base.get_databases()
        return database
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“åˆ—è¡¨å¤±è´¥ {e}, {traceback.format_exc()}")
        return {"message": f"è·å–æ•°æ®åº“åˆ—è¡¨å¤±è´¥ {e}", "databases": []}


@knowledge.post("/databases")
async def create_database(
    database_name: str = Body(...),
    description: str = Body(...),
    embed_model_name: str = Body(...),
    kb_type: str = Body("lightrag"),
    additional_params: dict = Body({}),
    llm_info: dict = Body(None),
    current_user: User = Depends(get_admin_user),
):
    """åˆ›å»ºçŸ¥è¯†åº“"""
    logger.debug(
        f"Create database {database_name} with kb_type {kb_type}, "
        f"additional_params {additional_params}, llm_info {llm_info}, "
        f"embed_model_name {embed_model_name}"
    )
    try:
        # å…ˆæ£€æŸ¥åç§°æ˜¯å¦å·²å­˜åœ¨
        if knowledge_base.database_name_exists(database_name):
            raise HTTPException(
                status_code=409,
                detail=f"çŸ¥è¯†åº“åç§° '{database_name}' å·²å­˜åœ¨ï¼Œè¯·ä½¿ç”¨å…¶ä»–åç§°",
            )

        additional_params = {**(additional_params or {})}
        additional_params["auto_generate_questions"] = False  # é»˜è®¤ä¸ç”Ÿæˆé—®é¢˜

        def remove_reranker_config(kb: str, params: dict) -> None:
            """
            ç§»é™¤ reranker_configï¼ˆå·²åºŸå¼ƒï¼‰
            æ‰€æœ‰ reranker å‚æ•°ç°åœ¨é€šè¿‡ query_params.options é…ç½®
            """
            reranker_cfg = params.get("reranker_config")
            if reranker_cfg:
                if kb == "milvus":
                    logger.info("reranker_config is deprecated, please use query_params.options instead")
                else:
                    logger.warning(f"{kb} does not support reranker, ignoring reranker_config")
                # ç§»é™¤ reranker_configï¼Œä¸å†ä¿å­˜
                params.pop("reranker_config", None)

        remove_reranker_config(kb_type, additional_params)

        embed_info = config.embed_model_names[embed_model_name]
        database_info = await knowledge_base.create_database(
            database_name, description, kb_type=kb_type, embed_info=embed_info, llm_info=llm_info, **additional_params
        )

        # éœ€è¦é‡æ–°åŠ è½½æ‰€æœ‰æ™ºèƒ½ä½“ï¼Œå› ä¸ºå·¥å…·åˆ·æ–°äº†
        from src.agents import agent_manager

        await agent_manager.reload_all()

        return database_info
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ›å»ºæ•°æ®åº“å¤±è´¥ {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=400, detail=f"åˆ›å»ºæ•°æ®åº“å¤±è´¥: {e}")


@knowledge.get("/databases/{db_id}")
async def get_database_info(db_id: str, current_user: User = Depends(get_admin_user)):
    """è·å–çŸ¥è¯†åº“è¯¦ç»†ä¿¡æ¯"""
    database = knowledge_base.get_database_info(db_id)
    if database is None:
        raise HTTPException(status_code=404, detail="Database not found")
    return database


@knowledge.put("/databases/{db_id}")
async def update_database_info(
    db_id: str,
    name: str = Body(...),
    description: str = Body(...),
    llm_info: dict = Body(None),
    additional_params: dict = Body({}),  # Now accepts a dict
    current_user: User = Depends(get_admin_user),
):
    """æ›´æ–°çŸ¥è¯†åº“ä¿¡æ¯"""
    logger.debug(
        f"Update database {db_id} info: {name}, {description}, llm_info: {llm_info}, "
        f"additional_params: {additional_params}"
    )
    try:
        database = await knowledge_base.update_database(
            db_id,
            name,
            description,
            llm_info,
            additional_params=additional_params,  # Pass the dict to the manager
        )
        return {"message": "æ›´æ–°æˆåŠŸ", "database": database}
    except Exception as e:
        logger.error(f"æ›´æ–°æ•°æ®åº“å¤±è´¥ {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=400, detail=f"æ›´æ–°æ•°æ®åº“å¤±è´¥: {e}")


@knowledge.delete("/databases/{db_id}")
async def delete_database(db_id: str, current_user: User = Depends(get_admin_user)):
    """åˆ é™¤çŸ¥è¯†åº“"""
    logger.debug(f"Delete database {db_id}")
    try:
        await knowledge_base.delete_database(db_id)

        # éœ€è¦é‡æ–°åŠ è½½æ‰€æœ‰æ™ºèƒ½ä½“ï¼Œå› ä¸ºå·¥å…·åˆ·æ–°äº†
        from src.agents import agent_manager

        await agent_manager.reload_all()

        return {"message": "åˆ é™¤æˆåŠŸ"}
    except Exception as e:
        logger.error(f"åˆ é™¤æ•°æ®åº“å¤±è´¥ {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=400, detail=f"åˆ é™¤æ•°æ®åº“å¤±è´¥: {e}")


@knowledge.get("/databases/{db_id}/export")
async def export_database(
    db_id: str,
    format: str = Query("csv", enum=["csv", "xlsx", "md", "txt"]),
    include_vectors: bool = Query(False, description="æ˜¯å¦åœ¨å¯¼å‡ºä¸­åŒ…å«å‘é‡æ•°æ®"),
    current_user: User = Depends(get_admin_user),
):
    """å¯¼å‡ºçŸ¥è¯†åº“æ•°æ®"""
    logger.debug(f"Exporting database {db_id} with format {format}")
    try:
        file_path = await knowledge_base.export_data(db_id, format=format, include_vectors=include_vectors)

        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="Exported file not found.")

        media_type = media_types.get(format, "application/octet-stream")

        return FileResponse(path=file_path, filename=os.path.basename(file_path), media_type=media_type)
    except NotImplementedError as e:
        logger.warning(f"A disabled feature was accessed: {e}")
        raise HTTPException(status_code=501, detail=str(e))
    except Exception as e:
        logger.error(f"å¯¼å‡ºæ•°æ®åº“å¤±è´¥ {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºæ•°æ®åº“å¤±è´¥: {e}")


# =============================================================================
# === çŸ¥è¯†åº“æ–‡æ¡£ç®¡ç†åˆ†ç»„ ===
# =============================================================================


@knowledge.post("/databases/{db_id}/documents")
async def add_documents(
    db_id: str, items: list[str] = Body(...), params: dict = Body(...), current_user: User = Depends(get_admin_user)
):
    """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼ˆä¸Šä¼  -> è§£æ -> å¯é€‰å…¥åº“ï¼‰"""
    logger.debug(f"Add documents for db_id {db_id}: {items} {params=}")

    content_type = params.get("content_type", "file")
    # è‡ªåŠ¨å…¥åº“å‚æ•°
    auto_index = params.get("auto_index", False)
    indexing_params = {
        "chunk_size": params.get("chunk_size", 1000),
        "chunk_overlap": params.get("chunk_overlap", 200),
        "qa_separator": params.get("qa_separator", ""),
    }

    # ç¦æ­¢ URL è§£æä¸å…¥åº“
    if content_type == "url":
        raise HTTPException(status_code=400, detail="URL æ–‡æ¡£ä¸Šä¼ ä¸è§£æå·²ç¦ç”¨")

    # å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯æ–‡ä»¶è·¯å¾„
    if content_type == "file":
        from src.knowledge.utils.kb_utils import validate_file_path

        for item in items:
            try:
                validate_file_path(item, db_id)
            except ValueError as e:
                raise HTTPException(status_code=403, detail=str(e))

    async def run_ingest(context: TaskContext):
        await context.set_message("ä»»åŠ¡åˆå§‹åŒ–")
        await context.set_progress(5.0, "å‡†å¤‡å¤„ç†æ–‡æ¡£")

        total = len(items)
        processed_items = []

        # å­˜å‚¨ç¬¬ä¸€é˜¶æ®µæˆåŠŸæ·»åŠ çš„æ–‡ä»¶è®°å½• {item: (file_id, file_meta)}
        added_files = {}

        try:
            # ========== ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡æ·»åŠ æ–‡ä»¶è®°å½• ==========
            await context.set_message("ç¬¬ä¸€é˜¶æ®µï¼šæ·»åŠ æ–‡ä»¶è®°å½•")
            for idx, item in enumerate(items, 1):
                await context.raise_if_cancelled()

                # ç¬¬ä¸€é˜¶æ®µè¿›åº¦ï¼š5% ~ 30%
                progress = 5.0 + (idx / total) * 25.0
                await context.set_progress(progress, f"[1/2] æ·»åŠ è®°å½• {idx}/{total}")

                try:
                    # 1. Add file record (UPLOADED)
                    file_meta = await knowledge_base.add_file_record(
                        db_id, item, params=params, operator_id=current_user.id
                    )
                    file_id = file_meta["file_id"]
                    added_files[item] = (file_id, file_meta)
                except Exception as add_error:
                    logger.error(f"æ·»åŠ æ–‡ä»¶è®°å½•å¤±è´¥ {item}: {add_error}")
                    error_type = "timeout" if isinstance(add_error, TimeoutError) else "add_failed"
                    error_msg = "æ·»åŠ è¶…æ—¶" if isinstance(add_error, TimeoutError) else "æ·»åŠ è®°å½•å¤±è´¥"
                    processed_items.append(
                        {
                            "item": item,
                            "status": "failed",
                            "error": f"{error_msg}: {str(add_error)}",
                            "error_type": error_type,
                        }
                    )

            # ========== ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡è§£ææ–‡ä»¶ ==========
            await context.set_message("ç¬¬äºŒé˜¶æ®µï¼šè§£ææ–‡ä»¶")
            parse_success_count = 0
            # è®¡ç®—è§£æé˜¶æ®µçš„è¿›åº¦èŒƒå›´
            parse_progress_range = 30.0 if not auto_index else 25.0

            for idx, (item, (file_id, add_file_meta)) in enumerate(added_files.items(), 1):
                await context.raise_if_cancelled()

                # ç¬¬äºŒé˜¶æ®µè¿›åº¦ï¼š25%~55% æˆ– 30%~60%
                progress = parse_progress_range + (idx / len(added_files)) * 30.0
                await context.set_progress(progress, f"[2/2] è§£ææ–‡ä»¶ {idx}/{len(added_files)}")

                try:
                    # 2. Parse file (PARSING -> PARSED)
                    file_meta = await knowledge_base.parse_file(db_id, file_id, operator_id=current_user.id)
                    processed_items.append(file_meta)
                    parse_success_count += 1
                except Exception as parse_error:
                    logger.error(f"è§£ææ–‡ä»¶å¤±è´¥ {item} (file_id={file_id}): {parse_error}")
                    error_type = "timeout" if isinstance(parse_error, TimeoutError) else "parse_failed"
                    error_msg = "è§£æè¶…æ—¶" if isinstance(parse_error, TimeoutError) else "è§£æå¤±è´¥"
                    processed_items.append(
                        {
                            "item": item,
                            "status": "failed",
                            "error": f"{error_msg}: {str(parse_error)}",
                            "error_type": error_type,
                        }
                    )

            # ========== ç¬¬ä¸‰é˜¶æ®µï¼šè‡ªåŠ¨å…¥åº“ ==========
            if auto_index:
                await context.set_message("ç¬¬ä¸‰é˜¶æ®µï¼šè‡ªåŠ¨å…¥åº“")
                parsed_files = [(item, data) for item, data in added_files.items() if data[1].get("status") == "parsed"]
                total_parsed = len(parsed_files)

                for idx, (item, (file_id, file_meta)) in enumerate(parsed_files, 1):
                    await context.raise_if_cancelled()

                    # ç¬¬ä¸‰é˜¶æ®µè¿›åº¦ï¼š55%~95% æˆ– 60%~95%
                    progress = 55.0 + (idx / total_parsed) * 40.0
                    await context.set_progress(progress, f"[3/3] å…¥åº“æ–‡ä»¶ {idx}/{total_parsed}")

                    try:
                        # 1. æ›´æ–°å…¥åº“å‚æ•°
                        await knowledge_base.update_file_params(
                            db_id, file_id, indexing_params, operator_id=current_user.id
                        )
                        # 2. æ‰§è¡Œå…¥åº“
                        result = await knowledge_base.index_file(db_id, file_id, operator_id=current_user.id)
                        processed_items.append(result)
                    except Exception as index_error:
                        logger.error(f"è‡ªåŠ¨å…¥åº“å¤±è´¥ {item} (file_id={file_id}): {index_error}")
                        processed_items.append(
                            {
                                "item": item,
                                "status": "failed",
                                "error": f"å…¥åº“å¤±è´¥: {str(index_error)}",
                                "error_type": "index_failed",
                            }
                        )

        except asyncio.CancelledError:
            await context.set_progress(100.0, "ä»»åŠ¡å·²å–æ¶ˆ")
            raise
        except Exception as task_error:
            # å¤„ç†æ•´ä½“ä»»åŠ¡çš„å…¶ä»–å¼‚å¸¸ï¼ˆå¦‚å†…å­˜ä¸è¶³ã€ç½‘ç»œé”™è¯¯ç­‰ï¼‰
            logger.exception(f"Task processing failed: {task_error}")
            await context.set_progress(100.0, f"ä»»åŠ¡å¤„ç†å¤±è´¥: {str(task_error)}")
            # æ³¨æ„ï¼šä¸éœ€è¦æ‰‹åŠ¨æ ‡è®°æœªå¤„ç†çš„æ–‡ä»¶ä¸ºå¤±è´¥ï¼Œå› ä¸ºï¼š
            # 1. å†…å±‚å¼‚å¸¸å¤„ç†å·²è®°å½•æ‰€æœ‰å¤„ç†è¿‡çš„æ–‡ä»¶ï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰
            # 2. æœªå¤„ç†çš„æ–‡ä»¶æ²¡æœ‰è¿›å…¥ processed_itemsï¼Œå‰ç«¯ä¼šæ­£ç¡®æ˜¾ç¤º
            # 3. ç”¨æˆ·å¯ä»¥é‡æ–°æäº¤æœªå¤„ç†çš„æ–‡ä»¶
            raise

        item_type = "URL" if content_type == "url" else "æ–‡ä»¶"
        # Check for failed status (including ERROR_PARSING)
        failed_count = len([_p for _p in processed_items if "error" in _p or _p.get("status") == "failed"])

        summary = {
            "db_id": db_id,
            "item_type": item_type,
            "submitted": len(processed_items),
            "failed": failed_count,
        }
        message = f"{item_type}å¤„ç†å®Œæˆï¼Œå¤±è´¥ {failed_count} ä¸ª" if failed_count else f"{item_type}å¤„ç†å®Œæˆ"
        await context.set_result(summary | {"items": processed_items})
        await context.set_progress(100.0, message)
        
        # Auto-generate mindmap if files were successfully processed
        success_count = len(processed_items) - failed_count
        if success_count > 0:
            try:
                logger.info(f"Auto-generating mindmap for database {db_id} after processing {success_count} files")
                asyncio.create_task(_auto_generate_mindmap(db_id))
            except Exception as mindmap_error:
                logger.warning(f"Failed to trigger auto mindmap generation: {mindmap_error}")
                # Don't fail the main task if mindmap generation fails
        
        return summary | {"items": processed_items}

    try:
        database = knowledge_base.get_database_info(db_id)
        task = await tasker.enqueue(
            name=f"çŸ¥è¯†åº“æ–‡æ¡£å¤„ç† ({database['name']})",
            task_type="knowledge_ingest",
            payload={
                "db_id": db_id,
                "items": items,
                "params": params,
                "content_type": content_type,
            },
            coroutine=run_ingest,
        )
        return {
            "message": "ä»»åŠ¡å·²æäº¤ï¼Œè¯·åœ¨ä»»åŠ¡ä¸­å¿ƒæŸ¥çœ‹è¿›åº¦",
            "status": "queued",
            "task_id": task.id,
        }
    except Exception as e:  # noqa: BLE001
        logger.error(f"Failed to enqueue {content_type}s: {e}, {traceback.format_exc()}")
        return {"message": f"Failed to enqueue task: {e}", "status": "failed"}


@knowledge.post("/databases/{db_id}/documents/parse")
async def parse_documents(db_id: str, file_ids: list[str] = Body(...), current_user: User = Depends(get_admin_user)):
    """æ‰‹åŠ¨è§¦å‘æ–‡æ¡£è§£æ"""
    logger.debug(f"Parse documents for db_id {db_id}: {file_ids}")

    async def run_parse(context: TaskContext):
        await context.set_message("ä»»åŠ¡åˆå§‹åŒ–")
        await context.set_progress(5.0, "å‡†å¤‡è§£ææ–‡æ¡£")

        total = len(file_ids)
        processed_items = []

        try:
            for idx, file_id in enumerate(file_ids, 1):
                await context.raise_if_cancelled()
                progress = 5.0 + (idx / total) * 90.0
                await context.set_progress(progress, f"æ­£åœ¨è§£æç¬¬ {idx}/{total} ä¸ªæ–‡æ¡£")

                try:
                    result = await knowledge_base.parse_file(db_id, file_id, operator_id=current_user.id)
                    processed_items.append(result)
                except Exception as e:
                    logger.error(f"Parse failed for {file_id}: {e}")
                    processed_items.append({"file_id": file_id, "status": "failed", "error": str(e)})

        except Exception as e:
            logger.exception(f"Parse task failed: {e}")
            raise

        failed_count = len([p for p in processed_items if "error" in p])
        message = f"è§£æå®Œæˆï¼Œå¤±è´¥ {failed_count} ä¸ª"
        await context.set_result({"items": processed_items})
        await context.set_progress(100.0, message)
        return {"items": processed_items}

    try:
        database = knowledge_base.get_database_info(db_id)
        task = await tasker.enqueue(
            name=f"æ–‡æ¡£è§£æ ({database['name']})",
            task_type="knowledge_parse",
            payload={"db_id": db_id, "file_ids": file_ids},
            coroutine=run_parse,
        )
        return {"message": "è§£æä»»åŠ¡å·²æäº¤", "status": "queued", "task_id": task.id}
    except Exception as e:
        return {"message": f"æäº¤å¤±è´¥: {e}", "status": "failed"}


@knowledge.post("/databases/{db_id}/documents/index")
async def index_documents(
    db_id: str,
    file_ids: list[str] = Body(...),
    params: dict = Body({}),
    current_user: User = Depends(get_admin_user),
):
    """æ‰‹åŠ¨è§¦å‘æ–‡æ¡£å…¥åº“ï¼ˆIndexingï¼‰ï¼Œæ”¯æŒæ›´æ–°å‚æ•°"""
    logger.debug(f"Index documents for db_id {db_id}: {file_ids} {params=}")

    # extract operator_id safely before background task
    operator_id = current_user.id

    async def run_index(context: TaskContext):
        await context.set_message("ä»»åŠ¡åˆå§‹åŒ–")
        await context.set_progress(5.0, "å‡†å¤‡å…¥åº“æ–‡æ¡£")

        total = len(file_ids)
        processed_items = []

        # Track files that failed param update
        param_update_failed = set()

        try:
            # Update params if provided
            if params:
                for file_id in file_ids:
                    try:
                        await knowledge_base.update_file_params(db_id, file_id, params, operator_id=operator_id)
                    except Exception as e:
                        logger.error(f"Failed to update params for {file_id}: {e}")
                        param_update_failed.add(file_id)
                        processed_items.append(
                            {"file_id": file_id, "status": "failed", "error": f"å‚æ•°æ›´æ–°å¤±è´¥: {str(e)}"}
                        )

            for idx, file_id in enumerate(file_ids, 1):
                await context.raise_if_cancelled()

                # Skip files that failed param update
                if file_id in param_update_failed:
                    logger.debug(f"Skipping {file_id} due to param update failure")
                    continue

                progress = 5.0 + (idx / total) * 90.0
                await context.set_progress(progress, f"æ­£åœ¨å…¥åº“ç¬¬ {idx}/{total} ä¸ªæ–‡æ¡£")

                try:
                    result = await knowledge_base.index_file(db_id, file_id, operator_id=operator_id)
                    processed_items.append(result)
                except Exception as e:
                    logger.error(f"Index failed for {file_id}: {e}")
                    processed_items.append({"file_id": file_id, "status": "failed", "error": str(e)})

        except Exception as e:
            logger.exception(f"Index task failed: {e}")
            raise

        failed_count = len([p for p in processed_items if "error" in p])
        message = f"å…¥åº“å®Œæˆï¼Œå¤±è´¥ {failed_count} ä¸ª"
        await context.set_result({"items": processed_items})
        await context.set_progress(100.0, message)
        return {"items": processed_items}

    try:
        database = knowledge_base.get_database_info(db_id)
        task = await tasker.enqueue(
            name=f"æ–‡æ¡£å…¥åº“ ({database['name']})",
            task_type="knowledge_index",
            payload={"db_id": db_id, "file_ids": file_ids, "params": params},
            coroutine=run_index,
        )
        return {"message": "å…¥åº“ä»»åŠ¡å·²æäº¤", "status": "queued", "task_id": task.id}
    except Exception as e:
        return {"message": f"æäº¤å¤±è´¥: {e}", "status": "failed"}


@knowledge.get("/databases/{db_id}/documents/{doc_id}")
async def get_document_info(db_id: str, doc_id: str, current_user: User = Depends(get_admin_user)):
    """è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«åŸºæœ¬ä¿¡æ¯å’Œå†…å®¹ä¿¡æ¯ï¼‰"""
    logger.debug(f"GET document {doc_id} info in {db_id}")

    try:
        info = await knowledge_base.get_file_info(db_id, doc_id)
        return info
    except Exception as e:
        logger.error(f"Failed to get file info, {e}, {db_id=}, {doc_id=}, {traceback.format_exc()}")
        return {"message": "Failed to get file info", "status": "failed"}


@knowledge.get("/databases/{db_id}/documents/{doc_id}/basic")
async def get_document_basic_info(db_id: str, doc_id: str, current_user: User = Depends(get_admin_user)):
    """è·å–æ–‡æ¡£åŸºæœ¬ä¿¡æ¯ï¼ˆä»…å…ƒæ•°æ®ï¼‰"""
    logger.debug(f"GET document {doc_id} basic info in {db_id}")

    try:
        info = await knowledge_base.get_file_basic_info(db_id, doc_id)
        return info
    except Exception as e:
        logger.error(f"Failed to get file basic info, {e}, {db_id=}, {doc_id=}, {traceback.format_exc()}")
        return {"message": "Failed to get file basic info", "status": "failed"}


@knowledge.get("/databases/{db_id}/documents/{doc_id}/content")
async def get_document_content(db_id: str, doc_id: str, current_user: User = Depends(get_admin_user)):
    """è·å–æ–‡æ¡£å†…å®¹ä¿¡æ¯ï¼ˆchunkså’Œlinesï¼‰"""
    logger.debug(f"GET document {doc_id} content in {db_id}")

    try:
        info = await knowledge_base.get_file_content(db_id, doc_id)
        return info
    except Exception as e:
        logger.error(f"Failed to get file content, {e}, {db_id=}, {doc_id=}, {traceback.format_exc()}")
        return {"message": "Failed to get file content", "status": "failed"}


@knowledge.delete("/databases/{db_id}/documents/{doc_id}")
async def delete_document(db_id: str, doc_id: str, current_user: User = Depends(get_admin_user)):
    """åˆ é™¤æ–‡æ¡£æˆ–æ–‡ä»¶å¤¹"""
    logger.debug(f"DELETE document {doc_id} info in {db_id}")
    try:
        file_meta_info = await knowledge_base.get_file_basic_info(db_id, doc_id)

        # Check if it is a folder
        is_folder = file_meta_info.get("meta", {}).get("is_folder", False)
        if is_folder:
            await knowledge_base.delete_folder(db_id, doc_id)
            return {"message": "æ–‡ä»¶å¤¹åˆ é™¤æˆåŠŸ"}

        file_name = file_meta_info.get("meta", {}).get("filename")

        # å°è¯•ä»MinIOåˆ é™¤æ–‡ä»¶ï¼Œå¦‚æœå¤±è´¥ï¼ˆä¾‹å¦‚æ—§çŸ¥è¯†åº“æ²¡æœ‰MinIOå®ä¾‹ï¼‰ï¼Œåˆ™å¿½ç•¥
        try:
            minio_client = get_minio_client()
            await minio_client.adelete_file("ref-" + db_id.replace("_", "-"), file_name)
            logger.debug(f"æˆåŠŸä»MinIOåˆ é™¤æ–‡ä»¶: {file_name}")
        except Exception as minio_error:
            logger.warning(f"ä»MinIOåˆ é™¤æ–‡ä»¶å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æ—§çŸ¥è¯†åº“ï¼‰: {minio_error}")

        # æ— è®ºMinIOåˆ é™¤æ˜¯å¦æˆåŠŸï¼Œéƒ½ç»§ç»­ä»çŸ¥è¯†åº“åˆ é™¤
        await knowledge_base.delete_file(db_id, doc_id)
        return {"message": "åˆ é™¤æˆåŠŸ"}
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥ {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=400, detail=f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")


@knowledge.get("/databases/{db_id}/documents/{doc_id}/download")
async def download_document(db_id: str, doc_id: str, request: Request, current_user: User = Depends(get_admin_user)):
    """ä¸‹è½½åŸå§‹æ–‡ä»¶ - æ ¹æ®pathç±»å‹é€‰æ‹©æœ¬åœ°æˆ–MinIOä¸‹è½½"""
    logger.debug(f"Download document {doc_id} from {db_id}")
    try:
        file_info = await knowledge_base.get_file_basic_info(db_id, doc_id)
        file_meta = file_info.get("meta", {})

        # è·å–æ–‡ä»¶è·¯å¾„å’Œæ–‡ä»¶å
        file_path = file_meta.get("path", "")
        filename = file_meta.get("filename", "file")
        logger.debug(f"File path from database: {file_path}")
        logger.debug(f"Original filename from database: {filename}")

        # è§£ç URLç¼–ç çš„æ–‡ä»¶åï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        try:
            decoded_filename = unquote(filename, encoding="utf-8")
            logger.debug(f"Decoded filename: {decoded_filename}")
        except Exception as e:
            logger.debug(f"Failed to decode filename {filename}: {e}")
            decoded_filename = filename  # å¦‚æœè§£ç å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡ä»¶å

        _, ext = os.path.splitext(decoded_filename)
        media_type = media_types.get(ext.lower(), "application/octet-stream")

        # æ ¹æ®pathç±»å‹é€‰æ‹©ä¸‹è½½æ–¹å¼
        from src.knowledge.utils.kb_utils import is_minio_url

        if is_minio_url(file_path):
            # MinIOä¸‹è½½
            logger.debug(f"Downloading from MinIO: {file_path}")

            try:
                # ä½¿ç”¨é€šç”¨å‡½æ•°è§£æMinIO URL
                from src.knowledge.utils.kb_utils import parse_minio_url

                bucket_name, object_name = parse_minio_url(file_path)

                logger.debug(f"Parsed bucket_name: {bucket_name}, object_name: {object_name}")

                minio_client = get_minio_client()

                # ç›´æ¥ä½¿ç”¨è§£æå‡ºçš„å®Œæ•´å¯¹è±¡åç§°ä¸‹è½½
                minio_response = await minio_client.adownload_response(
                    bucket_name=bucket_name,
                    object_name=object_name,
                )
                logger.debug(f"Successfully downloaded object: {object_name}")

            except Exception as e:
                logger.error(f"Failed to download MinIO file: {e}")
                raise StorageError(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")

            # åˆ›å»ºæµå¼ç”Ÿæˆå™¨
            async def minio_stream():
                try:
                    while True:
                        chunk = await asyncio.to_thread(minio_response.read, 8192)
                        if not chunk:
                            break
                        yield chunk
                finally:
                    minio_response.close()
                    minio_response.release_conn()

            # åˆ›å»ºStreamingResponse
            response = StreamingResponse(
                minio_stream(),
                media_type=media_type,
            )
            # æ­£ç¡®å¤„ç†ä¸­æ–‡æ–‡ä»¶åçš„HTTPå¤´éƒ¨è®¾ç½®
            try:
                # å°è¯•ä½¿ç”¨ASCIIç¼–ç ï¼ˆé€‚ç”¨äºè‹±æ–‡æ–‡ä»¶åï¼‰
                decoded_filename.encode("ascii")
                # å¦‚æœæˆåŠŸï¼Œç›´æ¥ä½¿ç”¨ç®€å•æ ¼å¼
                response.headers["Content-Disposition"] = f'attachment; filename="{decoded_filename}"'
            except UnicodeEncodeError:
                # å¦‚æœåŒ…å«éASCIIå­—ç¬¦ï¼ˆå¦‚ä¸­æ–‡ï¼‰ï¼Œä½¿ç”¨RFC 2231æ ¼å¼
                encoded_filename = quote(decoded_filename.encode("utf-8"))
                response.headers["Content-Disposition"] = f"attachment; filename*=UTF-8''{encoded_filename}"

            return response

        else:
            # æœ¬åœ°æ–‡ä»¶ä¸‹è½½
            logger.debug(f"Downloading from local filesystem: {file_path}")

            if not os.path.exists(file_path):
                raise StorageError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)

            # åˆ›å»ºæ–‡ä»¶æµå¼ç”Ÿæˆå™¨
            async def file_stream():
                async with aiofiles.open(file_path, "rb") as f:
                    while True:
                        chunk = await f.read(8192)
                        if not chunk:
                            break
                        yield chunk

            # åˆ›å»ºStreamingResponse
            response = StreamingResponse(
                file_stream(),
                media_type=media_type,
            )
            # æ­£ç¡®å¤„ç†ä¸­æ–‡æ–‡ä»¶åçš„HTTPå¤´éƒ¨è®¾ç½®
            try:
                # å°è¯•ä½¿ç”¨ASCIIç¼–ç ï¼ˆé€‚ç”¨äºè‹±æ–‡æ–‡ä»¶åï¼‰
                decoded_filename.encode("ascii")
                # å¦‚æœæˆåŠŸï¼Œç›´æ¥ä½¿ç”¨ç®€å•æ ¼å¼
                response.headers["Content-Disposition"] = f'attachment; filename="{decoded_filename}"'
                response.headers["Content-Length"] = str(file_size)
            except UnicodeEncodeError:
                # å¦‚æœåŒ…å«éASCIIå­—ç¬¦ï¼ˆå¦‚ä¸­æ–‡ï¼‰ï¼Œä½¿ç”¨RFC 2231æ ¼å¼
                encoded_filename = quote(decoded_filename.encode("utf-8"))
                response.headers["Content-Disposition"] = f"attachment; filename*=UTF-8''{encoded_filename}"
                response.headers["Content-Length"] = str(file_size)

            return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½å¤±è´¥: {e}")


# =============================================================================
# === çŸ¥è¯†åº“æŸ¥è¯¢åˆ†ç»„ ===
# =============================================================================


@knowledge.post("/databases/{db_id}/query")
async def query_knowledge_base(
    db_id: str, query: str = Body(...), meta: dict = Body(...), current_user: User = Depends(get_admin_user)
):
    """æŸ¥è¯¢çŸ¥è¯†åº“"""
    logger.debug(f"Query knowledge base {db_id}: {query}")
    try:
        result = await knowledge_base.aquery(query, db_id=db_id, **meta)
        return {"result": result, "status": "success"}
    except Exception as e:
        logger.error(f"çŸ¥è¯†åº“æŸ¥è¯¢å¤±è´¥ {e}, {traceback.format_exc()}")
        return {"message": f"çŸ¥è¯†åº“æŸ¥è¯¢å¤±è´¥: {e}", "status": "failed"}


@knowledge.post("/databases/{db_id}/query-test")
async def query_test(
    db_id: str, query: str = Body(...), meta: dict = Body(...), current_user: User = Depends(get_admin_user)
):
    """æµ‹è¯•æŸ¥è¯¢çŸ¥è¯†åº“"""
    logger.debug(f"Query test in {db_id}: {query}")
    try:
        result = await knowledge_base.aquery(query, db_id=db_id, **meta)
        return result
    except Exception as e:
        logger.error(f"æµ‹è¯•æŸ¥è¯¢å¤±è´¥ {e}, {traceback.format_exc()}")
        return {"message": f"æµ‹è¯•æŸ¥è¯¢å¤±è´¥: {e}", "status": "failed"}


@knowledge.put("/databases/{db_id}/query-params")
async def update_knowledge_base_query_params(
    db_id: str, params: dict = Body(...), current_user: User = Depends(get_admin_user)
):
    """æ›´æ–°çŸ¥è¯†åº“æŸ¥è¯¢å‚æ•°é…ç½®"""
    try:
        # è·å–çŸ¥è¯†åº“å®ä¾‹
        kb_instance = knowledge_base.get_kb(db_id)
        if not kb_instance:
            raise HTTPException(status_code=404, detail="Knowledge base not found")

        # æ›´æ–°å®ä¾‹å…ƒæ•°æ®ä¸­çš„æŸ¥è¯¢å‚æ•°
        async with knowledge_base._metadata_lock:
            # ç¡®ä¿ db_id åœ¨å®ä¾‹çš„ databases_meta ä¸­
            if db_id not in kb_instance.databases_meta:
                raise HTTPException(status_code=404, detail="Database not found in instance metadata")

            # ä½¿ç”¨ setdefault ç®€åŒ–åµŒå¥—å­—å…¸çš„åˆå§‹åŒ–
            options = kb_instance.databases_meta[db_id].setdefault("query_params", {}).setdefault("options", {})
            options.update(params)
            kb_instance._save_metadata()

            logger.info(f"æ›´æ–°çŸ¥è¯†åº“ {db_id} æŸ¥è¯¢å‚æ•°: {params}")

        return {"message": "success", "data": params}

    except Exception as e:
        logger.error(f"æ›´æ–°çŸ¥è¯†åº“æŸ¥è¯¢å‚æ•°å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°æŸ¥è¯¢å‚æ•°å¤±è´¥: {str(e)}")


@knowledge.get("/databases/{db_id}/query-params")
async def get_knowledge_base_query_params(db_id: str, current_user: User = Depends(get_admin_user)):
    """è·å–çŸ¥è¯†åº“ç±»å‹ç‰¹å®šçš„æŸ¥è¯¢å‚æ•°"""
    try:
        # è·å–çŸ¥è¯†åº“å®ä¾‹
        kb_instance = knowledge_base._get_kb_for_database(db_id)

        # è°ƒç”¨çŸ¥è¯†åº“å®ä¾‹çš„æ–¹æ³•è·å–é…ç½®
        params = kb_instance.get_query_params_config(
            db_id=db_id,
            reranker_names=config.reranker_names,  # ä¼ é€’åŠ¨æ€é…ç½®
        )

        # è·å–ç”¨æˆ·ä¿å­˜çš„é…ç½®å¹¶åˆå¹¶ï¼ˆä»å®ä¾‹ metadata è¯»å–ï¼‰
        saved_options = kb_instance._get_query_params(db_id)
        if saved_options:
            params = _merge_saved_options(params, saved_options)

        return {"params": params, "message": "success"}

    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“æŸ¥è¯¢å‚æ•°å¤±è´¥ {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


def _merge_saved_options(params: dict, saved_options: dict) -> dict:
    """å°†ç”¨æˆ·ä¿å­˜çš„é…ç½®åˆå¹¶åˆ°é»˜è®¤é…ç½®ä¸­"""
    for option in params.get("options", []):
        key = option.get("key")
        if key in saved_options:
            option["default"] = saved_options[key]
    return params


# =============================================================================
# === AIç”Ÿæˆç¤ºä¾‹é—®é¢˜ ===
# =============================================================================


SAMPLE_QUESTIONS_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“é—®ç­”æµ‹è¯•ä¸“å®¶ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®çŸ¥è¯†åº“ä¸­çš„æ–‡ä»¶åˆ—è¡¨ï¼Œç”Ÿæˆæœ‰ä»·å€¼çš„æµ‹è¯•é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. é—®é¢˜è¦å…·ä½“ã€æœ‰é’ˆå¯¹æ€§ï¼ŒåŸºäºæ–‡ä»¶åç§°å’Œç±»å‹æ¨æµ‹å¯èƒ½çš„å†…å®¹
2. é—®é¢˜è¦æ¶µç›–ä¸åŒæ–¹é¢å’Œéš¾åº¦
3. é—®é¢˜è¦ç®€æ´æ˜äº†ï¼Œé€‚åˆç”¨äºæ£€ç´¢æµ‹è¯•
4. é—®é¢˜è¦å¤šæ ·åŒ–ï¼ŒåŒ…æ‹¬äº‹å®æŸ¥è¯¢ã€æ¦‚å¿µè§£é‡Šã€æ“ä½œæŒ‡å¯¼ç­‰
5. é—®é¢˜é•¿åº¦æ§åˆ¶åœ¨10-30å­—ä¹‹é—´
6. ç›´æ¥è¿”å›JSONæ•°ç»„æ ¼å¼ï¼Œä¸è¦å…¶ä»–è¯´æ˜

è¿”å›æ ¼å¼ï¼š
```json
{
  "questions": [
    "é—®é¢˜1ï¼Ÿ",
    "é—®é¢˜2ï¼Ÿ",
    "é—®é¢˜3ï¼Ÿ"
  ]
}
```
"""


@knowledge.post("/databases/{db_id}/sample-questions")
async def generate_sample_questions(
    db_id: str,
    request_body: dict = Body(...),
    current_user: User = Depends(get_admin_user),
):
    """
    AIç”Ÿæˆé’ˆå¯¹çŸ¥è¯†åº“çš„æµ‹è¯•é—®é¢˜

    Args:
        db_id: çŸ¥è¯†åº“ID
        request_body: è¯·æ±‚ä½“ï¼ŒåŒ…å« count å­—æ®µ

    Returns:
        ç”Ÿæˆçš„é—®é¢˜åˆ—è¡¨
    """
    try:
        import json

        from src.models import select_model

        # ä»è¯·æ±‚ä½“ä¸­æå–å‚æ•°
        count = request_body.get("count", 10)

        # è·å–çŸ¥è¯†åº“ä¿¡æ¯
        db_info = knowledge_base.get_database_info(db_id)
        if not db_info:
            raise HTTPException(status_code=404, detail=f"çŸ¥è¯†åº“ {db_id} ä¸å­˜åœ¨")

        db_name = db_info.get("name", "")
        all_files = db_info.get("files", {})

        if not all_files:
            raise HTTPException(status_code=400, detail="çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡ä»¶")

        # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
        files_info = []
        for file_id, file_info in all_files.items():
            files_info.append(
                {
                    "filename": file_info.get("filename", ""),
                    "type": file_info.get("type", ""),
                }
            )

        # æ„å»ºAIæç¤ºè¯
        system_prompt = SAMPLE_QUESTIONS_SYSTEM_PROMPT

        # æ„å»ºç”¨æˆ·æ¶ˆæ¯
        files_text = "\n".join(
            [
                f"- {f['filename']} ({f['type']})"
                for f in files_info[:20]  # æœ€å¤šåˆ—ä¸¾20ä¸ªæ–‡ä»¶
            ]
        )

        file_count_text = f"ï¼ˆå…±{len(files_info)}ä¸ªæ–‡ä»¶ï¼‰" if len(files_info) > 20 else ""

        user_message = textwrap.dedent(f"""è¯·ä¸ºçŸ¥è¯†åº“"{db_name}"ç”Ÿæˆ{count}ä¸ªæµ‹è¯•é—®é¢˜ã€‚

            çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨{file_count_text}ï¼š
            {files_text}

            è¯·æ ¹æ®è¿™äº›æ–‡ä»¶çš„åç§°å’Œç±»å‹ï¼Œç”Ÿæˆ{count}ä¸ªæœ‰ä»·å€¼çš„æµ‹è¯•é—®é¢˜ã€‚""")

        # è°ƒç”¨AIç”Ÿæˆ
        logger.info(f"å¼€å§‹ç”ŸæˆçŸ¥è¯†åº“é—®é¢˜ï¼ŒçŸ¥è¯†åº“: {db_name}, æ–‡ä»¶æ•°é‡: {len(files_info)}, é—®é¢˜æ•°é‡: {count}")

        # é€‰æ‹©æ¨¡å‹å¹¶è°ƒç”¨
        model = select_model()
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_message}]
        response = model.call(messages, stream=False)

        # è§£æAIè¿”å›çš„JSON
        try:
            # æå–JSONå†…å®¹
            content = response.content if hasattr(response, "content") else str(response)

            # å°è¯•ä»markdownä»£ç å—ä¸­æå–JSON
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                content = content[json_start:json_end].strip()
            elif "```" in content:
                json_start = content.find("```") + 3
                json_end = content.find("```", json_start)
                content = content[json_start:json_end].strip()

            questions_data = json.loads(content)
            questions = questions_data.get("questions", [])

            if not questions or not isinstance(questions, list):
                raise ValueError("AIè¿”å›çš„é—®é¢˜æ ¼å¼ä¸æ­£ç¡®")

            logger.info(f"æˆåŠŸç”Ÿæˆ{len(questions)}ä¸ªé—®é¢˜")

            # ä¿å­˜é—®é¢˜åˆ°çŸ¥è¯†åº“å…ƒæ•°æ®
            try:
                async with knowledge_base._metadata_lock:
                    # ç¡®ä¿çŸ¥è¯†åº“å…ƒæ•°æ®å­˜åœ¨
                    if db_id not in knowledge_base.global_databases_meta:
                        knowledge_base.global_databases_meta[db_id] = {}
                    # ä¿å­˜é—®é¢˜åˆ°å¯¹åº”çŸ¥è¯†åº“
                    knowledge_base.global_databases_meta[db_id]["sample_questions"] = questions
                    knowledge_base._save_global_metadata()
                    logger.info(f"æˆåŠŸä¿å­˜ {len(questions)} ä¸ªé—®é¢˜åˆ°çŸ¥è¯†åº“ {db_id}")
            except Exception as save_error:
                logger.error(f"ä¿å­˜é—®é¢˜å¤±è´¥: {save_error}")

            return {
                "message": "success",
                "questions": questions,
                "count": len(questions),
                "db_id": db_id,
                "db_name": db_name,
            }

        except json.JSONDecodeError as e:
            logger.error(f"AIè¿”å›çš„JSONè§£æå¤±è´¥: {e}, åŸå§‹å†…å®¹: {content}")
            raise HTTPException(status_code=500, detail=f"AIè¿”å›æ ¼å¼é”™è¯¯: {str(e)}")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç”ŸæˆçŸ¥è¯†åº“é—®é¢˜å¤±è´¥: {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆé—®é¢˜å¤±è´¥: {str(e)}")


@knowledge.get("/databases/{db_id}/sample-questions")
async def get_sample_questions(db_id: str, current_user: User = Depends(get_admin_user)):
    """
    è·å–çŸ¥è¯†åº“çš„æµ‹è¯•é—®é¢˜

    Args:
        db_id: çŸ¥è¯†åº“ID

    Returns:
        é—®é¢˜åˆ—è¡¨
    """
    try:
        # ç›´æ¥ä»å…¨å±€å…ƒæ•°æ®ä¸­è¯»å–
        if db_id not in knowledge_base.global_databases_meta:
            raise HTTPException(status_code=404, detail=f"çŸ¥è¯†åº“ {db_id} ä¸å­˜åœ¨")

        db_meta = knowledge_base.global_databases_meta[db_id]
        questions = db_meta.get("sample_questions", [])

        return {
            "message": "success",
            "questions": questions,
            "count": len(questions),
            "db_id": db_id,
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“é—®é¢˜å¤±è´¥: {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"è·å–é—®é¢˜å¤±è´¥: {str(e)}")


# =============================================================================
# === æ–‡ä»¶ç®¡ç†åˆ†ç»„ ===
# =============================================================================


@knowledge.post("/databases/{db_id}/folders")
async def create_folder(
    db_id: str,
    folder_name: str = Body(..., embed=True),
    parent_id: str | None = Body(None, embed=True),
    current_user: User = Depends(get_admin_user),
):
    """åˆ›å»ºæ–‡ä»¶å¤¹"""
    try:
        return await knowledge_base.create_folder(db_id, folder_name, parent_id)
    except Exception as e:
        logger.error(f"åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥ {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


@knowledge.put("/databases/{db_id}/documents/{doc_id}/move")
async def move_document(
    db_id: str,
    doc_id: str,
    new_parent_id: str | None = Body(..., embed=True),
    current_user: User = Depends(get_admin_user),
):
    """ç§»åŠ¨æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹"""
    logger.debug(f"Move document {doc_id} to {new_parent_id} in {db_id}")
    try:
        return await knowledge_base.move_file(db_id, doc_id, new_parent_id)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"ç§»åŠ¨æ–‡ä»¶å¤±è´¥ {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


@knowledge.post("/files/upload")
async def upload_file(
    file: UploadFile = File(...),
    db_id: str | None = Query(None),
    allow_jsonl: bool = Query(False),
    current_user: User = Depends(get_admin_user),
):
    """ä¸Šä¼ æ–‡ä»¶"""
    if not file.filename:
        raise HTTPException(status_code=400, detail="No selected file")

    logger.debug(f"Received upload file with filename: {file.filename}")

    ext = os.path.splitext(file.filename)[1].lower()

    if ext == ".jsonl":
        if allow_jsonl is not True or db_id is not None:
            raise HTTPException(status_code=400, detail=f"Unsupported file type: {ext}")
    elif not (is_supported_file_extension(file.filename) or ext == ".zip"):
        raise HTTPException(status_code=400, detail=f"Unsupported file type: {ext}")

    basename, ext = os.path.splitext(file.filename)
    # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼ˆå°å†™ï¼‰
    filename = f"{basename}{ext}".lower()

    file_bytes = await file.read()

    content_hash = await calculate_content_hash(file_bytes)

    file_exists = await knowledge_base.file_existed_in_db(db_id, content_hash)
    if file_exists:
        raise HTTPException(
            status_code=409,
            detail="æ•°æ®åº“ä¸­å·²ç»å­˜åœ¨äº†ç›¸åŒå†…å®¹æ–‡ä»¶ï¼ŒFile with the same content already exists in this database",
        )

    # ç›´æ¥ä¸Šä¼ åˆ°MinIOï¼Œæ·»åŠ æ—¶é—´æˆ³åŒºåˆ†ç‰ˆæœ¬
    import time

    timestamp = int(time.time() * 1000)
    minio_filename = f"{basename}_{timestamp}{ext}"

    # ç”Ÿæˆç¬¦åˆMinIOè§„èŒƒçš„å­˜å‚¨æ¡¶åç§°ï¼ˆå°†ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºè¿å­—ç¬¦ï¼‰
    if db_id:
        bucket_name = f"ref-{db_id.replace('_', '-')}"
    else:
        bucket_name = "default-uploads"

    # ä¸Šä¼ åˆ°MinIO
    minio_url = await aupload_file_to_minio(bucket_name, minio_filename, file_bytes, ext.lstrip("."))

    # æ£€æµ‹åŒåæ–‡ä»¶ï¼ˆåŸºäºåŸå§‹æ–‡ä»¶åï¼‰
    same_name_files = await knowledge_base.get_same_name_files(db_id, filename)
    has_same_name = len(same_name_files) > 0

    return {
        "message": "File successfully uploaded",
        "file_path": minio_url,  # MinIOè·¯å¾„ä½œä¸ºä¸»è¦è·¯å¾„
        "minio_path": minio_url,  # MinIOè·¯å¾„
        "db_id": db_id,
        "content_hash": content_hash,
        "filename": filename,  # åŸå§‹æ–‡ä»¶åï¼ˆå°å†™ï¼‰
        "original_filename": basename,  # åŸå§‹æ–‡ä»¶åï¼ˆå»æ‰åç¼€ï¼‰
        "minio_filename": minio_filename,  # MinIOä¸­çš„æ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        "bucket_name": bucket_name,  # MinIOå­˜å‚¨æ¡¶åç§°
        "same_name_files": same_name_files,  # åŒåæ–‡ä»¶åˆ—è¡¨
        "has_same_name": has_same_name,  # æ˜¯å¦åŒ…å«åŒåæ–‡ä»¶æ ‡å¿—
    }


@knowledge.get("/files/supported-types")
async def get_supported_file_types(current_user: User = Depends(get_admin_user)):
    """è·å–å½“å‰æ”¯æŒçš„æ–‡ä»¶ç±»å‹"""
    return {"message": "success", "file_types": sorted(SUPPORTED_FILE_EXTENSIONS)}


@knowledge.post("/files/markdown")
async def mark_it_down(file: UploadFile = File(...), current_user: User = Depends(get_admin_user)):
    """è°ƒç”¨ src.knowledge.indexing ä¸‹é¢çš„ process_file_to_markdown è§£æä¸º markdownï¼Œå‚æ•°æ˜¯æ–‡ä»¶ï¼Œéœ€è¦ç®¡ç†å‘˜æƒé™"""
    try:
        content = await file.read()
        markdown_content = await process_file_to_markdown(content)
        return {"markdown_content": markdown_content, "message": "success"}
    except Exception as e:
        logger.error(f"æ–‡ä»¶è§£æå¤±è´¥ {e}, {traceback.format_exc()}")
        return {"message": f"æ–‡ä»¶è§£æå¤±è´¥ {e}", "markdown_content": ""}


# =============================================================================
# === çŸ¥è¯†åº“ç±»å‹åˆ†ç»„ ===
# =============================================================================


@knowledge.get("/types")
async def get_knowledge_base_types(current_user: User = Depends(get_admin_user)):
    """è·å–æ”¯æŒçš„çŸ¥è¯†åº“ç±»å‹"""
    try:
        kb_types = knowledge_base.get_supported_kb_types()
        return {"kb_types": kb_types, "message": "success"}
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“ç±»å‹å¤±è´¥ {e}, {traceback.format_exc()}")
        return {"message": f"è·å–çŸ¥è¯†åº“ç±»å‹å¤±è´¥ {e}", "kb_types": {}}


@knowledge.get("/stats")
async def get_knowledge_base_statistics(current_user: User = Depends(get_admin_user)):
    """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = knowledge_base.get_statistics()
        return {"stats": stats, "message": "success"}
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“ç»Ÿè®¡å¤±è´¥ {e}, {traceback.format_exc()}")
        return {"message": f"è·å–çŸ¥è¯†åº“ç»Ÿè®¡å¤±è´¥ {e}", "stats": {}}


# =============================================================================
# === Embeddingæ¨¡å‹çŠ¶æ€æ£€æŸ¥åˆ†ç»„ ===
# =============================================================================


@knowledge.get("/embedding-models/{model_id}/status")
async def get_embedding_model_status(model_id: str, current_user: User = Depends(get_admin_user)):
    """è·å–æŒ‡å®šembeddingæ¨¡å‹çš„çŠ¶æ€"""
    logger.debug(f"Checking embedding model status: {model_id}")
    try:
        status = await test_embedding_model_status(model_id)
        return {"status": status, "message": "success"}
    except Exception as e:
        logger.error(f"è·å–embeddingæ¨¡å‹çŠ¶æ€å¤±è´¥ {model_id}: {e}, {traceback.format_exc()}")
        return {
            "message": f"è·å–embeddingæ¨¡å‹çŠ¶æ€å¤±è´¥: {e}",
            "status": {"model_id": model_id, "status": "error", "message": str(e)},
        }


@knowledge.get("/embedding-models/status")
async def get_all_embedding_models_status(current_user: User = Depends(get_admin_user)):
    """è·å–æ‰€æœ‰embeddingæ¨¡å‹çš„çŠ¶æ€"""
    logger.debug("Checking all embedding models status")
    try:
        status = await test_all_embedding_models_status()
        return {"status": status, "message": "success"}
    except Exception as e:
        logger.error(f"è·å–æ‰€æœ‰embeddingæ¨¡å‹çŠ¶æ€å¤±è´¥: {e}, {traceback.format_exc()}")
        return {"message": f"è·å–æ‰€æœ‰embeddingæ¨¡å‹çŠ¶æ€å¤±è´¥: {e}", "status": {"models": {}, "total": 0, "available": 0}}


# =============================================================================
# === çŸ¥è¯†åº“ AI è¾…åŠ©åŠŸèƒ½åˆ†ç»„ ===
# =============================================================================


@knowledge.post("/generate-description")
async def generate_description(
    name: str = Body(..., description="çŸ¥è¯†åº“åç§°"),
    current_description: str = Body("", description="å½“å‰æè¿°ï¼ˆå¯é€‰ï¼Œç”¨äºä¼˜åŒ–ï¼‰"),
    file_list: list[str] = Body([], description="æ–‡ä»¶åˆ—è¡¨"),
    current_user: User = Depends(get_admin_user),
):
    """ä½¿ç”¨ LLM ç”Ÿæˆæˆ–ä¼˜åŒ–çŸ¥è¯†åº“æè¿°

    æ ¹æ®çŸ¥è¯†åº“åç§°å’Œç°æœ‰æè¿°ï¼Œä½¿ç”¨ LLM ç”Ÿæˆé€‚åˆä½œä¸ºæ™ºèƒ½ä½“å·¥å…·æè¿°çš„å†…å®¹ã€‚
    """
    from src.models import select_model

    logger.debug(f"Generating description for knowledge base: {name}, files: {len(file_list)}")

    # æ„å»ºæ–‡ä»¶åˆ—è¡¨æ–‡æœ¬
    if file_list:
        # é™åˆ¶æ–‡ä»¶æ•°é‡ï¼Œé¿å… prompt è¿‡é•¿
        display_files = file_list[:50]
        files_str = "\n".join([f"- {f}" for f in display_files])
        more_text = f"\n... (è¿˜æœ‰ {len(file_list) - 50} ä¸ªæ–‡ä»¶)" if len(file_list) > 50 else ""
        current_description += f"\n\nçŸ¥è¯†åº“åŒ…å«çš„æ–‡ä»¶:\n{files_str}{more_text}"

    current_description = current_description or "æš‚æ— æè¿°"

    # æ„å»ºæç¤ºè¯
    prompt = textwrap.dedent(f"""
        è¯·å¸®æˆ‘ä¼˜åŒ–ä»¥ä¸‹çŸ¥è¯†åº“çš„æè¿°ã€‚

        çŸ¥è¯†åº“åç§°: {name}
        å½“å‰æè¿°: {current_description}

        è¦æ±‚:
        1. è¿™ä¸ªæè¿°å°†ä½œä¸ºæ™ºèƒ½ä½“å·¥å…·çš„æè¿°ä½¿ç”¨
        2. æ™ºèƒ½ä½“ä¼šæ ¹æ®çŸ¥è¯†åº“çš„æ ‡é¢˜å’Œæè¿°æ¥é€‰æ‹©åˆé€‚çš„å·¥å…·
        3. æ‰€ä»¥æè¿°éœ€è¦æ¸…æ™°ã€å…·ä½“ï¼Œè¯´æ˜è¯¥çŸ¥è¯†åº“åŒ…å«ä»€ä¹ˆå†…å®¹ã€é€‚åˆè§£ç­”ä»€ä¹ˆç±»å‹çš„é—®é¢˜
        4. æè¿°åº”è¯¥ç®€æ´æœ‰åŠ›ï¼Œé€šå¸¸ 2-4 å¥è¯å³å¯
        5. ä¸è¦ä½¿ç”¨ Markdown æ ¼å¼
        {"6. è¯·å‚è€ƒæä¾›çš„æ–‡ä»¶åˆ—è¡¨æ¥å‡†ç¡®æ¦‚æ‹¬çŸ¥è¯†åº“å†…å®¹" if file_list else ""}

        è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„æè¿°ï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€è¯´æ˜ã€‚
    """).strip()

    try:
        model = select_model()
        response = await asyncio.to_thread(model.call, prompt)
        description = response.content.strip()
        logger.debug(f"Generated description: {description}")
        return {"description": description, "status": "success"}
    except Exception as e:
        logger.error(f"ç”Ÿæˆæè¿°å¤±è´¥: {e}, {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæè¿°å¤±è´¥: {e}")
